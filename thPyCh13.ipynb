{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 13 - Case study: data structure selection. \n",
    "At this point, we've seen many of Python's core data structures, and seen some ways to use them.  \n",
    "For other ways to use them, you may read:\n",
    "https://greenteapress.com/thinkpython2/html/thinkpython2022.html#algorithms\n",
    "\n",
    "This chapter presents a case study to practice choosing useful data structures for a task and then using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit to Allen Downey; this case study comes from https://greenteapress.com/thinkpython2/html/thinkpython2014.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Frequency Analysis\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exercise 1  \n",
    "Write a program that reads a file, breaks each line into words, strips whitespace and punctuation from the words, and converts them to lowercase.\n",
    "Hint: The string module provides a string named whitespace, which contains space, tab, newline, etc., and punctuation which contains the punctuation characters. Let’s see if we can make Python swear:\n",
    "\n",
    ">>> import string\n",
    ">>> string.punctuation\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "Also, you might consider using the string methods strip, replace and translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_file(filename):\n",
    "    hist = dict()\n",
    "    fp = open(filename)\n",
    "    for line in fp:\n",
    "        process_line(line, hist)\n",
    "    return hist\n",
    "\n",
    "def process_line(line, hist):\n",
    "    line = line.replace('-', ' ')\n",
    "    \n",
    "    for word in line.split():\n",
    "        word = word.strip(string.punctuation + string.whitespace)\n",
    "        word = word.lower()\n",
    "        hist[word] = hist.get(word, 0) + 1\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exercise 2  \n",
    "Go to Project Gutenberg (http://gutenberg.org) and download your favorite out-of-copyright book in plain text format.\n",
    "Modify your program from the previous exercise to read the book you downloaded, skip over the header information at the beginning of the file, and process the rest of the words as before.\n",
    "\n",
    "Then modify the program to count the total number of words in the book, and the number of times each word is used.\n",
    "\n",
    "Print the number of different words used in the book. Compare different books by different authors, written in different eras. Which author uses the most extensive vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '65113-0.webarchive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64fd40d6f2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'65113-0.webarchive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-64fd40d6f2e5>\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprocess_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '65113-0.webarchive'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def process_file(filename):\n",
    "    hist = dict()\n",
    "    fp = open(filename)\n",
    "    for line in fp:\n",
    "        process_line(line, hist)\n",
    "    return hist\n",
    "\n",
    "def process_line(line, hist):\n",
    "    line = line.replace('-', ' ')\n",
    "    \n",
    "    for word in line.split():\n",
    "        word = word.strip(string.punctuation + string.whitespace)\n",
    "        word = word.lower()\n",
    "        hist[word] = hist.get(word, 0) + 1\n",
    "\n",
    "hist = process_file('65113-0.webarchive')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exercise 3  \n",
    "Modify the program from the previous exercise to print the 20 most frequently used words in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line, hist):\n",
    "    \n",
    "\n",
    "    line = line.replace('-', ' ')\n",
    "    strippables = string.punctuation + string.whitespace\n",
    "\n",
    "    for word in line.split():\n",
    "    \n",
    "        word = word.strip(strippables)\n",
    "        word = word.lower()\n",
    "\n",
    "        hist[word] = hist.get(word, 1) + 2\n",
    "\n",
    "\n",
    "def most_common(hist):\n",
    "\n",
    "    t = []\n",
    "    for key, value in hist.items():\n",
    "        t.append((value, key))\n",
    "\n",
    "    t.sort()\n",
    "    t.reverse()\n",
    "    return t\n",
    "\n",
    "\n",
    "def print_most_common(hist, num=40):\n",
    "   \n",
    "    t = most_common(hist)\n",
    "    print('The most common words are:')\n",
    "    for freq, word in t[:num]:\n",
    "        print(word, '\\t', freq)\n",
    "\n",
    "\n",
    "def subtract(s1, s2):\n",
    "   \n",
    "    res = {}\n",
    "    for key in d1:\n",
    "        if key not s2:\n",
    "            res[key] = None\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exercise 4  \n",
    "Modify the previous program to read a word list (see Section 9.1) and then print all the words in the book that are not in the word list. How many of them are typos? How many of them are common words that should be in the word list, and how many of them are really obscure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    hist = process_file('158-0.txt', skip_header=True)\n",
    "    print('Total number of words:', total_words(hist))\n",
    "    print('Number of different words:', different_words(hist))\n",
    "\n",
    "    t = most_common(hist)\n",
    "    print('The most common words are:')\n",
    "    for freq, word in t[0:20]:\n",
    "        print(word, '\\t', freq)\n",
    "\n",
    "    words = process_file('words.txt', skip_header=False)\n",
    "\n",
    "    diff = subtract(hist, words)\n",
    "    print(\"The words in the book that aren't in the word list are:\")\n",
    "    for word in diff.keys():\n",
    "        print(word, end=' ')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read 13.2. \n",
    "\n",
    "Exercise 5\n",
    "\n",
    "Write a function named choose_from_hist that takes a histogram as defined in Section 11.2 and returns a random value from the histogram, chosen with probability in proportion to frequency. For example, for this histogram:\n",
    ">>> t = ['a', 'a', 'b']\n",
    ">>> hist = histogram(t)\n",
    ">>> hist\n",
    "{'a': 2, 'b': 1}\n",
    "your function should return 'a' with probability 2/3 and 'b' with probability 1/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read 13.3 - 13.6 (see also web links)\n",
    "Exercise 6  \n",
    "Python provides a data structure called set that provides many common set operations. You can read about them in Section 19.5, or read the documentation at http://docs.python.org/3/library/stdtypes.html#types-set.\n",
    "Write a program that uses set subtraction to find words in the book that are not in the word list. Solution: http://thinkpython2.com/code/analyze_book2.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read 13.7\n",
    "\n",
    "Challenge (not required but if you want to stretch your skills):\n",
    "Exercise 7  \n",
    "Write a program that uses this algorithm to choose a random word from the book. Solution: http://thinkpython2.com/code/analyze_book3.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read 13.8 \n",
    "\n",
    "Challenge (not required)\n",
    "\n",
    "Exercise 8  \n",
    "Markov analysis:\n",
    "Write a program to read a text from a file and perform Markov analysis. The result should be a dictionary that maps from prefixes to a collection of possible suffixes. The collection might be a list, tuple, or dictionary; it is up to you to make an appropriate choice. You can test your program with prefix length two, but you should write the program in a way that makes it easy to try other lengths.\n",
    "Add a function to the previous program to generate random text based on the Markov analysis. Here is an example from Emma with prefix length 2:\n",
    "He was very clever, be it sweetness or be angry, ashamed or only amused, at such a stroke. She had never thought of Hannah till you were never meant for me?\" \"I cannot make speeches, Emma:\" he soon cut it all himself.\n",
    "For this example, I left the punctuation attached to the words. The result is almost syntactically correct, but not quite. Semantically, it almost makes sense, but not quite.\n",
    "What happens if you increase the prefix length? Does the random text make more sense?\n",
    "\n",
    "Once your program is working, you might want to try a mash-up: if you combine text from two or more books, the random text you generate will blend the vocabulary and phrases from the sources in interesting ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Read 13.9 \n",
    "\n",
    "\n",
    "Challenge problem (not required):\n",
    "Exercise 9  \n",
    "The “rank” of a word is its position in a list of words sorted by frequency: the most common word has rank 1, the second most common has rank 2, etc.\n",
    "Zipf’s law describes a relationship between the ranks and frequencies of words in natural languages (http://en.wikipedia.org/wiki/Zipf's_law). Specifically, it predicts that the frequency, f, of the word with rank r is:\n",
    "\n",
    "f = c r−s \n",
    "where s and c are parameters that depend on the language and the text. If you take the logarithm of both sides of this equation, you get:\n",
    "logf = logc − s logr \n",
    "So if you plot log f versus log r, you should get a straight line with slope −s and intercept log c.\n",
    "Write a program that reads a text from a file, counts word frequencies, and prints one line for each word, in descending order of frequency, with log f and log r. Use the graphing program of your choice to plot the results and check whether they form a straight line. Can you estimate the value of s?\n",
    "\n",
    "Solution: http://thinkpython2.com/code/zipf.py. To run my solution, you need the plotting module matplotlib. If you installed Anaconda, you already have matplotlib; otherwise you might have to install it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
